version: 0.0.3
jobId: "493"
jobName: read_csv_pyspark_redshift_sadp
jobType: Source Aligned Data Product
domain: sale
alias: redshift_state_mgnt1_34
discoveryPort:
  name: read_csv_pyspark_redshift_sadp_Vishnu_23
inputPorts:
  - alias: read_csv_generic_1
    isDynamic: true
    path: s3://byte-etl-externaldemo/University_Admsn_Data/csv_files/university_admission.csv
    optional:
      persistDataFrame: false
      advanceOptions:
        delimiter: ","
      enableDataReconciliation: false
      enforceSchema: false
      connection: read_csv_s3_conn
      dataSetUrn: urn:dv:dataset:9e45f6b1-cac8-45d8-a09d-169e6326b101
    type: inputDelimited
productState:
  persistDataFrame: false
  enableDataReconciliation: false
  enforceSchema: false
  stepName: redshift_state_mgnt1
  connection: temp_redshift
  query: select * from public.byte_etl_library_redshift_test_new
  endPoint: s3.us-east-1.amazonaws.com
  temporaryPathS3: s3://bp-spark-sql-library-test-acc/testRedshiftUpload1/
  type: readRedshiftTableByQuery
  isStateManagement: true
  sequence: 3
  alias: redshift_state_mgnt1
  refreshInterval: None
  retentionVersions: ""
  logicalSchema:
    properties:
      City:
        type: STRING
        description: ""
      Region:
        type: STRING
        description: ""
      Country:
        type: STRING
        description: ""
      Latitude:
        type: STRING
        description: ""
      Longitude:
        type: STRING
        description: ""
      Timezone:
        type: STRING
        description: ""
      Local Time:
        type: STRING
        description: ""
  enforceSchemaMethod: ""
  isProfilingEnabled: false
transformation:
  - isDynamic: true
    alias: EMR_PySpark_1
    description: RunningPySparkJob
    sequence: 2
    arguments:
      - s3://byte-etl-externaldemo/University_Admsn_Data/csv_files/university_admission.csv
    pythonFilePath: s3://byte-etl-externaldemo/University_Admsn_Data/test.py
    inputDataFrameList:
      - inputDataFrame: ""
        tempViewName: ""
    optional:
      pythonEnvTarGZPath: s3://byte-etl-externaldemo/University_Admsn_Data/pyspark_venv.tar.gz
    type: customPySparkEMRServerless
controlPort:
  dataQualityRules: {}
outputPort:
  subscriptionChannels:
    - channelType: Postgres
      queryType: SQL
    - channelType: Dataproduct
      queryType: SQL
